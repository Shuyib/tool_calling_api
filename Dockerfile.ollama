# Dockerfile for Ollama API server with Qwen model
# Build: docker build -t ollama-qwen:0.1 -f Dockerfile.ollama .
# credit: DevFest Pwani 2024 in Kenya. Presentation Inference Your LLMs on the fly: Serverless Cloud Run with GPU Acceleration
# https://jochen.kirstaetter.name/

FROM ollama/ollama:0.6.8

# Metadata
LABEL maintainer="Shuyib" \
    description="Ollama API server with Qwen model" \
    version="0.1"

# Add user
RUN adduser --system --group ollama

# Set workdir
WORKDIR /app

# Set environment variables
ENV OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_MODELS=/models \
    OLLAMA_DEBUG=false \
    OLLAMA_KEEP_ALIVE=-1 \
    MODEL=qwen3:0.6b

# Create models directory
RUN mkdir -p /models && \
    chown -R ollama:ollama /models

# Switch to root to install small utilities and add entrypoint
USER root

# Install curl for healthchecks (assume Debian-based image); if the base image
# is different this will need adjusting. Keep installs minimal and clean up.
RUN apt-get update \
        && apt-get install -y --no-install-recommends curl \
        && rm -rf /var/lib/apt/lists/*

# Create a lightweight entrypoint script that starts ollama in the background,
# waits for the server to be ready, pulls the required model if missing, then
# waits on the server process. Running pull at container start ensures the
# model is placed in the container's /models volume.
RUN mkdir -p /usr/local/bin && cat > /usr/local/bin/ollama-entrypoint.sh <<'EOF'
#!/bin/sh
set -eu

# Start ollama server in background
ollama serve &
PID=$!

# Wait for server readiness (max ~60s)
COUNT=0
while [ $COUNT -lt 60 ]; do
    if curl -sSf http://127.0.0.1:11434/api/version >/dev/null 2>&1; then
        break
    fi
    COUNT=$((COUNT+1))
    sleep 1
done

# Attempt to pull model if it's not already listed
if ! ollama list 2>/dev/null | grep -q "qwen3:0.6b"; then
    echo "Pulling model qwen3:0.6b"
    # Allow pull failures to not kill container but log them
    ollama pull qwen3:0.6b || echo "Model pull failed; continue and let operator inspect logs"
fi

# Wait for the server process to exit
wait $PID
EOF

RUN chmod +x /usr/local/bin/ollama-entrypoint.sh && chown ollama:ollama /usr/local/bin/ollama-entrypoint.sh

# Revert to running as the ollama user for security
USER ollama

# Expose port
EXPOSE 11434

# Healthcheck: use curl which we installed
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
        CMD curl -f http://localhost:11434/api/version > /dev/null || exit 1

# Entrypoint: the wrapper script will start server and pull model
ENTRYPOINT ["/usr/local/bin/ollama-entrypoint.sh"]
